{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FastAPI:\n",
    "\n",
    "- FastAPI framework, high performance, easy to learn, fast to code, ready for production\n",
    "- Web framework for building APIs with Python 3.8+ based on standard Python type hints.\n",
    "- Interactive API docs (provided by Swagger UI)\n",
    "- Validation of data:\n",
    "Automatic and clear errors when the data is invalid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.get(\"/hello/{name}\")\n",
    "async def hello(name:str):\n",
    "    return f\"Hello {name} \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Asynchronous Code:\n",
    "\n",
    "Asynchronous code just means that the language üí¨ has a way to tell the computer / program ü§ñ that at some point in the code, \n",
    "it ü§ñ will have to wait for something else to finish somewhere else. Let's say that something else is called \"slow-file\" üìù.\n",
    "\n",
    "So, during that time, the computer can go and do some other work, while \"slow-file\" üìù finishes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'LLMs' : ['OpenAI', 'Mistral'],\n",
    "    'NLP' : ['Bert', 'RoBerta'],\n",
    "    'ML' : ['Xgboost', 'Catboost']\n",
    "}\n",
    "\n",
    "@app.get(\"/get_models/{usecase}\")\n",
    "async def get_items(usecase:str):\n",
    "    return models.get(usecase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Validation:\n",
    "\n",
    "class AvailableModel(str, Enum):\n",
    "    LLMs = \"LLMs\"\n",
    "    NLP = \"NLP\"\n",
    "    ML = \"ML\"\n",
    "    \n",
    "@app.get(\"/get_models/{usecase}\")\n",
    "async def get_items(usecase: AvailableModel):\n",
    "    return models.get(usecase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Item(BaseModel):\n",
    "    name: str\n",
    "    description: str | None = None\n",
    "    price: float\n",
    "    tax: float | None = None\n",
    "\n",
    "@app.post(\"/items/\")\n",
    "async def create_item(item: Item):\n",
    "    return item\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\DELL\\\\Desktop\\\\RAG_LLM_FASTAPI_PLAY\\\\model\\\\mistral-7b-instruct-v0.1.Q4_K_M.gguf'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getenv('MODEL_PATH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getenv(\"EMBED_MODEL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.llm import LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "llms = LLM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\Desktop\\RAG_LLM_FASTAPI_PLAY\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load the model: 3.366804599761963\n"
     ]
    }
   ],
   "source": [
    "local_mistal = llms.get_llm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\Desktop\\RAG_LLM_FASTAPI_PLAY\\.venv\\Lib\\site-packages\\langchain_together\\llms.py:93: UserWarning: The completions endpoint, has 'max_tokens' as required argument. The default value is being set to 200 Consider setting this value, when initializing LLM\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "t_mistral = llms.get_llm_together()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Together(together_api_key=SecretStr('**********'), model='mistralai/Mistral-7B-Instruct-v0.2', max_tokens=200)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_mistral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.build_rag import RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\Desktop\\RAG_LLM_FASTAPI_PLAY\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\DELL\\Desktop\\RAG_LLM_FASTAPI_PLAY\\.venv\\Lib\\site-packages\\transformers\\utils\\generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "rag = RAG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag = RAG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\Desktop\\RAG_LLM_FASTAPI_PLAY\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "rag.populate_vector_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = rag.get_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['Chroma', 'HuggingFaceBgeEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x000001E26FE51A30>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
